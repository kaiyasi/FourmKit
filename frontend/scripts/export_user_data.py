#!/usr/bin/env python3
"""
ä½¿ç”¨è€…è³‡æ–™åŒ¯å‡ºè…³æœ¬
ç”¨æ–¼åœ¨é‡å»ºç¶²ç«™å‰å‚™ä»½é‡è¦è³‡æ–™
"""

import os
import sys
import json
import csv
from datetime import datetime, timezone
from sqlalchemy import create_engine, text
from pathlib import Path

def export_user_data():
    """åŒ¯å‡ºä½¿ç”¨è€…è³‡æ–™"""
    
    # å˜—è©¦ä¸åŒçš„æ•¸æ“šåº« URL
    db_urls = [
        os.getenv('DATABASE_URL'),
        "postgresql+psycopg2://forumkit:forumkit@127.0.0.1:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@localhost:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@postgres:80/forumkit"
    ]
    
    engine = None
    for url in db_urls:
        if not url:
            continue
        try:
            print(f"ğŸ”§ å˜—è©¦é€£æ¥æ•¸æ“šåº«: {url}")
            engine = create_engine(url)
            # æ¸¬è©¦é€£æ¥
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print(f"âœ… æˆåŠŸé€£æ¥åˆ°æ•¸æ“šåº«: {url}")
            break
        except Exception as e:
            print(f"âŒ é€£æ¥å¤±æ•—: {e}")
            continue
    
    if not engine:
        print("âŒ ç„¡æ³•é€£æ¥åˆ°ä»»ä½•æ•¸æ“šåº«")
        print("ğŸ’¡ è«‹ç¢ºä¿ Docker å®¹å™¨æ­£åœ¨é‹è¡Œï¼šdocker-compose up -d")
        return
    
    # å‰µå»ºåŒ¯å‡ºç›®éŒ„
    export_dir = Path("exports")
    export_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        with engine.connect() as conn:
            print("ğŸ”§ é–‹å§‹åŒ¯å‡ºä½¿ç”¨è€…è³‡æ–™...")
            
            # 1. åŒ¯å‡ºä½¿ç”¨è€…åŸºæœ¬è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºä½¿ç”¨è€…åŸºæœ¬è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, username, email, role, school_id, 
                    created_at, avatar_path, is_premium, premium_until
                FROM users 
                ORDER BY id
            """))
            
            users_data = []
            for row in result:
                users_data.append({
                    'id': row[0],
                    'username': row[1],
                    'email': row[2],
                    'role': row[3],
                    'school_id': row[4],
                    'created_at': row[5].isoformat() if row[5] else None,
                    'avatar_path': row[6],
                    'is_premium': row[7],
                    'premium_until': row[8].isoformat() if row[8] else None
                })
            
            # ä¿å­˜ç‚º JSON
            users_file = export_dir / f"users_{timestamp}.json"
            with open(users_file, 'w', encoding='utf-8') as f:
                json.dump(users_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä½¿ç”¨è€…è³‡æ–™å·²åŒ¯å‡ºåˆ°: {users_file}")
            
            # ä¿å­˜ç‚º CSV
            users_csv_file = export_dir / f"users_{timestamp}.csv"
            if users_data:
                with open(users_csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=users_data[0].keys())
                    writer.writeheader()
                    writer.writerows(users_data)
                print(f"âœ… ä½¿ç”¨è€…è³‡æ–™ CSV å·²åŒ¯å‡ºåˆ°: {users_csv_file}")
            
            # 2. åŒ¯å‡ºå­¸æ ¡è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºå­¸æ ¡è³‡æ–™...")
            result = conn.execute(text("""
                SELECT id, name, slug, description, created_at
                FROM schools 
                ORDER BY id
            """))
            
            schools_data = []
            for row in result:
                schools_data.append({
                    'id': row[0],
                    'name': row[1],
                    'slug': row[2],
                    'description': row[3],
                    'created_at': row[4].isoformat() if row[4] else None
                })
            
            schools_file = export_dir / f"schools_{timestamp}.json"
            with open(schools_file, 'w', encoding='utf-8') as f:
                json.dump(schools_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å­¸æ ¡è³‡æ–™å·²åŒ¯å‡ºåˆ°: {schools_file}")
            
            # 3. åŒ¯å‡ºè²¼æ–‡è³‡æ–™ï¼ˆä¸å«å…§å®¹ï¼Œåªä¿ç•™åŸºæœ¬è³‡è¨Šï¼‰
            print("ğŸ“‹ åŒ¯å‡ºè²¼æ–‡åŸºæœ¬è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, author_id, status, school_id, 
                    created_at, is_pinned, is_announcement, is_advertisement,
                    delete_request_count
                FROM posts 
                ORDER BY id
            """))
            
            posts_data = []
            for row in result:
                posts_data.append({
                    'id': row[0],
                    'author_id': row[1],
                    'status': row[2],
                    'school_id': row[3],
                    'created_at': row[4].isoformat() if row[4] else None,
                    'is_pinned': row[5],
                    'is_announcement': row[6],
                    'is_advertisement': row[7],
                    'delete_request_count': row[8]
                })
            
            posts_file = export_dir / f"posts_basic_{timestamp}.json"
            with open(posts_file, 'w', encoding='utf-8') as f:
                json.dump(posts_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… è²¼æ–‡åŸºæœ¬è³‡æ–™å·²åŒ¯å‡ºåˆ°: {posts_file}")
            
            # 4. åŒ¯å‡ºå…¬å‘Šè³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºå…¬å‘Šè³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, title, content, type, priority, is_active, is_pinned,
                    start_at, end_at, school_id, created_by, created_at
                FROM announcements 
                ORDER BY id
            """))
            
            announcements_data = []
            for row in result:
                announcements_data.append({
                    'id': row[0],
                    'title': row[1],
                    'content': row[2],
                    'type': row[3],
                    'priority': row[4],
                    'is_active': row[5],
                    'is_pinned': row[6],
                    'start_at': row[7].isoformat() if row[7] else None,
                    'end_at': row[8].isoformat() if row[8] else None,
                    'school_id': row[9],
                    'created_by': row[10],
                    'created_at': row[11].isoformat() if row[11] else None
                })
            
            announcements_file = export_dir / f"announcements_{timestamp}.json"
            with open(announcements_file, 'w', encoding='utf-8') as f:
                json.dump(announcements_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å…¬å‘Šè³‡æ–™å·²åŒ¯å‡ºåˆ°: {announcements_file}")
            
            # 5. åŒ¯å‡ºæ”¯æ´å·¥å–®è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºæ”¯æ´å·¥å–®è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, public_id, user_id, subject, status, category, priority,
                    created_at, updated_at, assigned_to
                FROM support_tickets 
                ORDER BY id
            """))
            
            tickets_data = []
            for row in result:
                tickets_data.append({
                    'id': row[0],
                    'public_id': row[1],
                    'user_id': row[2],
                    'subject': row[3],
                    'status': row[4],
                    'category': row[5],
                    'priority': row[6],
                    'created_at': row[7].isoformat() if row[7] else None,
                    'updated_at': row[8].isoformat() if row[8] else None,
                    'assigned_to': row[9]
                })
            
            tickets_file = export_dir / f"support_tickets_{timestamp}.json"
            with open(tickets_file, 'w', encoding='utf-8') as f:
                json.dump(tickets_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ”¯æ´å·¥å–®è³‡æ–™å·²åŒ¯å‡ºåˆ°: {tickets_file}")
            
            # 6. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            print("ğŸ“Š ç”Ÿæˆçµ±è¨ˆå ±å‘Š...")
            stats = {}
            
            # ä½¿ç”¨è€…çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM users"))
            stats['total_users'] = result.fetchone()[0]
            
            result = conn.execute(text("SELECT role, COUNT(*) FROM users GROUP BY role"))
            stats['users_by_role'] = {row[0]: row[1] for row in result}
            
            # è²¼æ–‡çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM posts"))
            stats['total_posts'] = result.fetchone()[0]
            
            result = conn.execute(text("SELECT status, COUNT(*) FROM posts GROUP BY status"))
            stats['posts_by_status'] = {row[0]: row[1] for row in result}
            
            # å…¬å‘Šçµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM announcements"))
            stats['total_announcements'] = result.fetchone()[0]
            
            # æ”¯æ´å·¥å–®çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM support_tickets"))
            stats['total_support_tickets'] = result.fetchone()[0]
            
            stats_file = export_dir / f"statistics_{timestamp}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"âœ… çµ±è¨ˆå ±å‘Šå·²åŒ¯å‡ºåˆ°: {stats_file}")
            
            # 7. ç”ŸæˆåŒ¯å‡ºæ‘˜è¦
            summary = {
                'export_timestamp': datetime.now(timezone.utc).isoformat(),
                'export_files': [
                    str(users_file),
                    str(schools_file),
                    str(posts_file),
                    str(announcements_file),
                    str(tickets_file),
                    str(stats_file)
                ],
                'statistics': stats,
                'notes': [
                    "æ­¤åŒ¯å‡ºåŒ…å«é‡è¦çš„ä½¿ç”¨è€…å’Œç®¡ç†è³‡æ–™",
                    "è²¼æ–‡å…§å®¹æœªåŒ…å«åœ¨åŸºæœ¬è³‡æ–™åŒ¯å‡ºä¸­",
                    "å¯†ç¢¼é›œæ¹Šå·²åŒ…å«åœ¨åŒ¯å‡ºä¸­ï¼Œé‡å»ºæ™‚å¯ä¿ç•™",
                    "å»ºè­°åœ¨é‡å»ºå‰å‚™ä»½æ•´å€‹ exports ç›®éŒ„"
                ]
            }
            
            summary_file = export_dir / f"export_summary_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… åŒ¯å‡ºæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
            
            print("\nğŸ‰ è³‡æ–™åŒ¯å‡ºå®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰æª”æ¡ˆå·²ä¿å­˜åˆ°: {export_dir}")
            print(f"ğŸ“Š çµ±è¨ˆæ‘˜è¦:")
            print(f"   - ç¸½ä½¿ç”¨è€…æ•¸: {stats['total_users']}")
            print(f"   - ç¸½è²¼æ–‡æ•¸: {stats['total_posts']}")
            print(f"   - ç¸½å…¬å‘Šæ•¸: {stats['total_announcements']}")
            print(f"   - ç¸½æ”¯æ´å·¥å–®æ•¸: {stats['total_support_tickets']}")
            print("\nğŸ’¡ é‡å»ºå»ºè­°:")
            print("   1. å‚™ä»½æ•´å€‹ exports ç›®éŒ„")
            print("   2. è¨˜éŒ„ç•¶å‰çš„ç’°å¢ƒè®Šæ•¸è¨­å®š")
            print("   3. å‚™ä»½ä¸Šå‚³çš„åª’é«”æª”æ¡ˆ (uploads ç›®éŒ„)")
            print("   4. é‡å»ºå¾Œå¯ä»¥åƒè€ƒåŒ¯å‡ºçš„è³‡æ–™é‡æ–°å»ºç«‹ä½¿ç”¨è€…å¸³æˆ¶")
            
    except Exception as e:
        print(f"âŒ åŒ¯å‡ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    export_user_data()
