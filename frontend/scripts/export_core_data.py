#!/usr/bin/env python3
"""
æ ¸å¿ƒè³‡æ–™åŒ¯å‡ºè…³æœ¬
åªåŒ¯å‡ºè²¼æ–‡ã€ä½¿ç”¨è€…ã€å­¸æ ¡è³‡æ–™
"""

import os
import sys
import json
import csv
from datetime import datetime, timezone
from sqlalchemy import create_engine, text
from pathlib import Path

def export_core_data():
    """åŒ¯å‡ºæ ¸å¿ƒè³‡æ–™ï¼šè²¼æ–‡ã€ä½¿ç”¨è€…ã€å­¸æ ¡"""
    
    # å˜—è©¦ä¸åŒçš„æ•¸æ“šåº« URL
    db_urls = [
        os.getenv('DATABASE_URL'),
        "postgresql+psycopg2://forumkit:forumkit@127.0.0.1:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@localhost:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@postgres:80/forumkit"
    ]
    
    engine = None
    for url in db_urls:
        if not url:
            continue
        try:
            print(f"ğŸ”§ å˜—è©¦é€£æ¥æ•¸æ“šåº«: {url}")
            engine = create_engine(url)
            # æ¸¬è©¦é€£æ¥
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print(f"âœ… æˆåŠŸé€£æ¥åˆ°æ•¸æ“šåº«: {url}")
            break
        except Exception as e:
            print(f"âŒ é€£æ¥å¤±æ•—: {e}")
            continue
    
    if not engine:
        print("âŒ ç„¡æ³•é€£æ¥åˆ°ä»»ä½•æ•¸æ“šåº«")
        print("ğŸ’¡ è«‹ç¢ºä¿ Docker å®¹å™¨æ­£åœ¨é‹è¡Œï¼šdocker-compose up -d")
        return
    
    # å‰µå»ºåŒ¯å‡ºç›®éŒ„
    export_dir = Path("exports")
    export_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        with engine.connect() as conn:
            print("ğŸ”§ é–‹å§‹åŒ¯å‡ºæ ¸å¿ƒè³‡æ–™...")
            
            # 1. åŒ¯å‡ºä½¿ç”¨è€…è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºä½¿ç”¨è€…è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, username, email, role, school_id, 
                    created_at, avatar_path, is_premium, premium_until,
                    password_hash
                FROM users 
                ORDER BY id
            """))
            
            users_data = []
            for row in result:
                users_data.append({
                    'id': row[0],
                    'username': row[1],
                    'email': row[2],
                    'role': row[3],
                    'school_id': row[4],
                    'created_at': row[5].isoformat() if row[5] else None,
                    'avatar_path': row[6],
                    'is_premium': row[7],
                    'premium_until': row[8].isoformat() if row[8] else None,
                    'password_hash': row[9]  # ä¿ç•™å¯†ç¢¼é›œæ¹Šä»¥ä¾¿é‡å»º
                })
            
            # ä¿å­˜ç‚º JSON
            users_file = export_dir / f"users_{timestamp}.json"
            with open(users_file, 'w', encoding='utf-8') as f:
                json.dump(users_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ä½¿ç”¨è€…è³‡æ–™å·²åŒ¯å‡ºåˆ°: {users_file}")
            
            # ä¿å­˜ç‚º CSV
            users_csv_file = export_dir / f"users_{timestamp}.csv"
            if users_data:
                with open(users_csv_file, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=users_data[0].keys())
                    writer.writeheader()
                    writer.writerows(users_data)
                print(f"âœ… ä½¿ç”¨è€…è³‡æ–™ CSV å·²åŒ¯å‡ºåˆ°: {users_csv_file}")
            
            # 2. åŒ¯å‡ºå­¸æ ¡è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºå­¸æ ¡è³‡æ–™...")
            result = conn.execute(text("""
                SELECT id, name, slug, description, created_at
                FROM schools 
                ORDER BY id
            """))
            
            schools_data = []
            for row in result:
                schools_data.append({
                    'id': row[0],
                    'name': row[1],
                    'slug': row[2],
                    'description': row[3],
                    'created_at': row[4].isoformat() if row[4] else None
                })
            
            schools_file = export_dir / f"schools_{timestamp}.json"
            with open(schools_file, 'w', encoding='utf-8') as f:
                json.dump(schools_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… å­¸æ ¡è³‡æ–™å·²åŒ¯å‡ºåˆ°: {schools_file}")
            
            # 3. åŒ¯å‡ºå®Œæ•´è²¼æ–‡è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºå®Œæ•´è²¼æ–‡è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, author_id, title, content, status, school_id, 
                    created_at, updated_at, is_pinned, is_announcement, 
                    is_advertisement, delete_request_count, view_count,
                    like_count, dislike_count, comment_count
                FROM posts 
                ORDER BY id
            """))
            
            posts_data = []
            for row in result:
                posts_data.append({
                    'id': row[0],
                    'author_id': row[1],
                    'title': row[2],
                    'content': row[3],
                    'status': row[4],
                    'school_id': row[5],
                    'created_at': row[6].isoformat() if row[6] else None,
                    'updated_at': row[7].isoformat() if row[7] else None,
                    'is_pinned': row[8],
                    'is_announcement': row[9],
                    'is_advertisement': row[10],
                    'delete_request_count': row[11],
                    'view_count': row[12],
                    'like_count': row[13],
                    'dislike_count': row[14],
                    'comment_count': row[15]
                })
            
            posts_file = export_dir / f"posts_{timestamp}.json"
            with open(posts_file, 'w', encoding='utf-8') as f:
                json.dump(posts_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… è²¼æ–‡è³‡æ–™å·²åŒ¯å‡ºåˆ°: {posts_file}")
            
            # 4. åŒ¯å‡ºç•™è¨€è³‡æ–™
            print("ğŸ“‹ åŒ¯å‡ºç•™è¨€è³‡æ–™...")
            result = conn.execute(text("""
                SELECT 
                    id, post_id, author_id, content, created_at, 
                    updated_at, is_deleted, parent_id, like_count, 
                    dislike_count
                FROM comments 
                ORDER BY id
            """))
            
            comments_data = []
            for row in result:
                comments_data.append({
                    'id': row[0],
                    'post_id': row[1],
                    'author_id': row[2],
                    'content': row[3],
                    'created_at': row[4].isoformat() if row[4] else None,
                    'updated_at': row[5].isoformat() if row[5] else None,
                    'is_deleted': row[6],
                    'parent_id': row[7],
                    'like_count': row[8],
                    'dislike_count': row[9]
                })
            
            comments_file = export_dir / f"comments_{timestamp}.json"
            with open(comments_file, 'w', encoding='utf-8') as f:
                json.dump(comments_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç•™è¨€è³‡æ–™å·²åŒ¯å‡ºåˆ°: {comments_file}")
            
            # 5. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            print("ğŸ“Š ç”Ÿæˆçµ±è¨ˆå ±å‘Š...")
            stats = {}
            
            # ä½¿ç”¨è€…çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM users"))
            stats['total_users'] = result.fetchone()[0]
            
            result = conn.execute(text("SELECT role, COUNT(*) FROM users GROUP BY role"))
            stats['users_by_role'] = {row[0]: row[1] for row in result}
            
            # å­¸æ ¡çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM schools"))
            stats['total_schools'] = result.fetchone()[0]
            
            # è²¼æ–‡çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM posts"))
            stats['total_posts'] = result.fetchone()[0]
            
            result = conn.execute(text("SELECT status, COUNT(*) FROM posts GROUP BY status"))
            stats['posts_by_status'] = {row[0]: row[1] for row in result}
            
            # ç•™è¨€çµ±è¨ˆ
            result = conn.execute(text("SELECT COUNT(*) FROM comments"))
            stats['total_comments'] = result.fetchone()[0]
            
            stats_file = export_dir / f"statistics_{timestamp}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"âœ… çµ±è¨ˆå ±å‘Šå·²åŒ¯å‡ºåˆ°: {stats_file}")
            
            # 6. ç”ŸæˆåŒ¯å‡ºæ‘˜è¦
            summary = {
                'export_timestamp': datetime.now(timezone.utc).isoformat(),
                'export_type': 'core_data_only',
                'export_files': [
                    str(users_file),
                    str(schools_file),
                    str(posts_file),
                    str(comments_file),
                    str(stats_file)
                ],
                'statistics': stats,
                'notes': [
                    "æ­¤åŒ¯å‡ºåƒ…åŒ…å«æ ¸å¿ƒè³‡æ–™ï¼šä½¿ç”¨è€…ã€å­¸æ ¡ã€è²¼æ–‡ã€ç•™è¨€",
                    "å¯†ç¢¼é›œæ¹Šå·²åŒ…å«åœ¨åŒ¯å‡ºä¸­ï¼Œé‡å»ºæ™‚å¯ä¿ç•™",
                    "è²¼æ–‡å…§å®¹å®Œæ•´ä¿ç•™",
                    "ç•™è¨€å…§å®¹å®Œæ•´ä¿ç•™",
                    "å»ºè­°åœ¨é‡å»ºå‰å‚™ä»½æ•´å€‹ exports ç›®éŒ„"
                ]
            }
            
            summary_file = export_dir / f"export_summary_{timestamp}.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            print(f"âœ… åŒ¯å‡ºæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
            
            print("\nğŸ‰ æ ¸å¿ƒè³‡æ–™åŒ¯å‡ºå®Œæˆï¼")
            print(f"ğŸ“ æ‰€æœ‰æª”æ¡ˆå·²ä¿å­˜åˆ°: {export_dir}")
            print(f"ğŸ“Š çµ±è¨ˆæ‘˜è¦:")
            print(f"   - ç¸½ä½¿ç”¨è€…æ•¸: {stats['total_users']}")
            print(f"   - ç¸½å­¸æ ¡æ•¸: {stats['total_schools']}")
            print(f"   - ç¸½è²¼æ–‡æ•¸: {stats['total_posts']}")
            print(f"   - ç¸½ç•™è¨€æ•¸: {stats['total_comments']}")
            print("\nğŸ’¡ é‡å»ºå»ºè­°:")
            print("   1. å‚™ä»½æ•´å€‹ exports ç›®éŒ„")
            print("   2. è¨˜éŒ„ç•¶å‰çš„ç’°å¢ƒè®Šæ•¸è¨­å®š")
            print("   3. é‡å»ºæ™‚å…ˆå»ºç«‹å­¸æ ¡ï¼Œå†å»ºç«‹ä½¿ç”¨è€…ï¼Œæœ€å¾Œå»ºç«‹è²¼æ–‡")
            print("   4. ç¢ºä¿è³‡æ–™åº«å¤–éµé—œä¿‚æ­£ç¢º")
            
    except Exception as e:
        print(f"âŒ åŒ¯å‡ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    export_core_data()
