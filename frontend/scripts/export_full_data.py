#!/usr/bin/env python3
"""
å®Œæ•´è³‡æ–™åŒ¯å‡ºè…³æœ¬
ç”¨æ–¼åœ¨é‡å»ºç¶²ç«™å‰å‚™ä»½æ‰€æœ‰é‡è¦è³‡æ–™
"""

import os
import sys
import json
import csv
import shutil
from datetime import datetime, timezone
from sqlalchemy import create_engine, text
from pathlib import Path

def export_full_data():
    """åŒ¯å‡ºå®Œæ•´è³‡æ–™"""
    
    # å˜—è©¦ä¸åŒçš„æ•¸æ“šåº« URL
    db_urls = [
        os.getenv('DATABASE_URL'),
        "postgresql+psycopg2://forumkit:forumkit@127.0.0.1:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@localhost:12007/forumkit",
        "postgresql+psycopg2://forumkit:forumkit@postgres:80/forumkit"
    ]
    
    engine = None
    for url in db_urls:
        if not url:
            continue
        try:
            print(f"ğŸ”§ å˜—è©¦é€£æ¥æ•¸æ“šåº«: {url}")
            engine = create_engine(url)
            # æ¸¬è©¦é€£æ¥
            with engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print(f"âœ… æˆåŠŸé€£æ¥åˆ°æ•¸æ“šåº«: {url}")
            break
        except Exception as e:
            print(f"âŒ é€£æ¥å¤±æ•—: {e}")
            continue
    
    if not engine:
        print("âŒ ç„¡æ³•é€£æ¥åˆ°ä»»ä½•æ•¸æ“šåº«")
        print("ğŸ’¡ è«‹ç¢ºä¿ Docker å®¹å™¨æ­£åœ¨é‹è¡Œï¼šdocker-compose up -d")
        return
    
    # å‰µå»ºåŒ¯å‡ºç›®éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    export_dir = Path(f"exports/full_backup_{timestamp}")
    export_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        with engine.connect() as conn:
            print("ğŸ”§ é–‹å§‹å®Œæ•´è³‡æ–™åŒ¯å‡º...")
            
            # 1. åŒ¯å‡ºæ‰€æœ‰è¡¨çµæ§‹
            print("ğŸ“‹ åŒ¯å‡ºè³‡æ–™åº«çµæ§‹...")
            result = conn.execute(text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                ORDER BY table_name
            """))
            
            tables = [row[0] for row in result]
            print(f"ğŸ“Š ç™¼ç¾ {len(tables)} å€‹è³‡æ–™è¡¨")
            
            # 2. åŒ¯å‡ºæ¯å€‹è¡¨çš„è³‡æ–™
            for table in tables:
                print(f"ğŸ“‹ åŒ¯å‡º {table} è³‡æ–™...")
                try:
                    # ç²å–è¡¨çµæ§‹
                    result = conn.execute(text(f"""
                        SELECT column_name, data_type, is_nullable
                        FROM information_schema.columns 
                        WHERE table_name = '{table}' 
                        AND table_schema = 'public'
                        ORDER BY ordinal_position
                    """))
                    
                    columns = [row[0] for row in result]
                    
                    # åŒ¯å‡ºè³‡æ–™
                    result = conn.execute(text(f"SELECT * FROM {table} ORDER BY id"))
                    
                    table_data = []
                    for row in result:
                        row_dict = {}
                        for i, value in enumerate(row):
                            if hasattr(value, 'isoformat'):  # è™•ç†æ—¥æœŸæ™‚é–“
                                row_dict[columns[i]] = value.isoformat()
                            else:
                                row_dict[columns[i]] = value
                        table_data.append(row_dict)
                    
                    # ä¿å­˜ç‚º JSON
                    table_file = export_dir / f"{table}.json"
                    with open(table_file, 'w', encoding='utf-8') as f:
                        json.dump(table_data, f, ensure_ascii=False, indent=2)
                    
                    print(f"   âœ… {table}: {len(table_data)} ç­†è¨˜éŒ„")
                    
                except Exception as e:
                    print(f"   âŒ {table}: åŒ¯å‡ºå¤±æ•— - {e}")
            
            # 3. å‚™ä»½ä¸Šå‚³æª”æ¡ˆ
            print("ğŸ“ å‚™ä»½ä¸Šå‚³æª”æ¡ˆ...")
            uploads_dir = Path("uploads")
            if uploads_dir.exists():
                backup_uploads_dir = export_dir / "uploads_backup"
                shutil.copytree(uploads_dir, backup_uploads_dir)
                print(f"âœ… ä¸Šå‚³æª”æ¡ˆå·²å‚™ä»½åˆ°: {backup_uploads_dir}")
            else:
                print("â„¹ï¸ uploads ç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éæª”æ¡ˆå‚™ä»½")
            
            # 4. å‚™ä»½ç’°å¢ƒè¨­å®š
            print("âš™ï¸ å‚™ä»½ç’°å¢ƒè¨­å®š...")
            env_files = [
                "backend/env.example",
                "backend/config/config.json",
                "docker-compose.yml",
                ".env"
            ]
            
            for env_file in env_files:
                if Path(env_file).exists():
                    backup_env_file = export_dir / f"config_{Path(env_file).name}"
                    shutil.copy2(env_file, backup_env_file)
                    print(f"âœ… {env_file} å·²å‚™ä»½")
            
            # 5. ç”Ÿæˆçµ±è¨ˆå ±å‘Š
            print("ğŸ“Š ç”Ÿæˆè©³ç´°çµ±è¨ˆå ±å‘Š...")
            stats = {
                'export_timestamp': datetime.now(timezone.utc).isoformat(),
                'database_tables': len(tables),
                'table_records': {}
            }
            
            for table in tables:
                try:
                    result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                    count = result.fetchone()[0]
                    stats['table_records'][table] = count
                except:
                    stats['table_records'][table] = 'error'
            
            stats_file = export_dir / "statistics.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            # 6. ç”Ÿæˆé‡å»ºæŒ‡å—
            print("ğŸ“ ç”Ÿæˆé‡å»ºæŒ‡å—...")
            rebuild_guide = f"""# ForumKit é‡å»ºæŒ‡å—

## åŒ¯å‡ºè³‡è¨Š
- åŒ¯å‡ºæ™‚é–“: {datetime.now(timezone.utc).isoformat()}
- è³‡æ–™åº«è¡¨æ•¸é‡: {len(tables)}
- ç¸½è¨˜éŒ„æ•¸: {sum(stats['table_records'].values()) if isinstance(sum(stats['table_records'].values()), int) else 'N/A'}

## é‡å»ºæ­¥é©Ÿ

### 1. ç’°å¢ƒæº–å‚™
```bash
# å…‹éš†å°ˆæ¡ˆ
git clone <your-repo>
cd ForumKit

# å®‰è£ä¾è³´
pip install -r backend/requirements.txt
npm install --prefix frontend
```

### 2. è³‡æ–™åº«é‡å»º
```bash
# å•Ÿå‹•è³‡æ–™åº«
docker-compose up -d postgres

# åŸ·è¡Œé·ç§»
cd backend
alembic upgrade head

# åŒ¯å…¥è³‡æ–™ï¼ˆå¯é¸ï¼‰
# ä½¿ç”¨åŒ¯å‡ºçš„ JSON æª”æ¡ˆé‡æ–°å»ºç«‹è³‡æ–™
```

### 3. ç’°å¢ƒè®Šæ•¸è¨­å®š
- è¤‡è£½ `config_env.example` åˆ° `.env`
- è¨­å®šå¿…è¦çš„ç’°å¢ƒè®Šæ•¸
- ç¢ºä¿ JWT_SECRET_KEY èˆ‡åŸç³»çµ±ç›¸åŒ

### 4. æª”æ¡ˆæ¢å¾©
- å°‡ `uploads_backup` ç›®éŒ„å…§å®¹è¤‡è£½åˆ° `uploads`
- ç¢ºä¿æª”æ¡ˆæ¬Šé™æ­£ç¢º

### 5. å•Ÿå‹•æœå‹™
```bash
# é–‹ç™¼æ¨¡å¼
docker-compose up

# æˆ–åˆ†åˆ¥å•Ÿå‹•
cd backend && python app.py
cd frontend && npm run dev
```

## æ³¨æ„äº‹é …
- å¯†ç¢¼é›œæ¹Šå·²åŒ…å«åœ¨åŒ¯å‡ºä¸­ï¼Œä½¿ç”¨è€…å¯ä»¥ä¿æŒåŸå¯†ç¢¼
- å»ºè­°åœ¨é‡å»ºå‰æ¸¬è©¦åŒ¯å‡ºè³‡æ–™çš„å®Œæ•´æ€§
- é‡å»ºå¾Œæª¢æŸ¥æ‰€æœ‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸é‹ä½œ

## åŒ¯å‡ºæª”æ¡ˆæ¸…å–®
"""
            
            for table in tables:
                rebuild_guide += f"- {table}.json\n"
            
            rebuild_guide += """
## è¯çµ¡è³‡è¨Š
å¦‚æœ‰å•é¡Œï¼Œè«‹è¯ç¹«ç³»çµ±ç®¡ç†å“¡ã€‚
"""
            
            guide_file = export_dir / "REBUILD_GUIDE.md"
            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write(rebuild_guide)
            
            print("âœ… é‡å»ºæŒ‡å—å·²ç”Ÿæˆ")
            
            # 7. ç”ŸæˆåŒ¯å‡ºæ‘˜è¦
            summary = {
                'export_timestamp': datetime.now(timezone.utc).isoformat(),
                'export_directory': str(export_dir),
                'database_tables': tables,
                'table_records': stats['table_records'],
                'files_backed_up': [
                    str(f) for f in export_dir.glob("*.json")
                ],
                'notes': [
                    "æ­¤ç‚ºå®Œæ•´è³‡æ–™å‚™ä»½ï¼ŒåŒ…å«æ‰€æœ‰è³‡æ–™åº«è¨˜éŒ„",
                    "ä¸Šå‚³æª”æ¡ˆå·²åŒ…å«åœ¨å‚™ä»½ä¸­",
                    "ç’°å¢ƒè¨­å®šæª”æ¡ˆå·²å‚™ä»½",
                    "é‡å»ºæ™‚è«‹åƒè€ƒ REBUILD_GUIDE.md"
                ]
            }
            
            summary_file = export_dir / "export_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print("\nğŸ‰ å®Œæ•´è³‡æ–™åŒ¯å‡ºå®Œæˆï¼")
            print(f"ğŸ“ å‚™ä»½ç›®éŒ„: {export_dir}")
            print(f"ğŸ“Š è³‡æ–™è¡¨æ•¸é‡: {len(tables)}")
            print(f"ğŸ“„ åŒ¯å‡ºæª”æ¡ˆ:")
            for table in tables:
                count = stats['table_records'].get(table, 0)
                print(f"   - {table}: {count} ç­†è¨˜éŒ„")
            
            print("\nğŸ’¡ é‡å»ºå»ºè­°:")
            print("   1. å‚™ä»½æ•´å€‹ exports ç›®éŒ„åˆ°å®‰å…¨ä½ç½®")
            print("   2. è¨˜éŒ„ç•¶å‰çš„ç’°å¢ƒè®Šæ•¸å’Œè¨­å®š")
            print("   3. æ¸¬è©¦åŒ¯å‡ºè³‡æ–™çš„å®Œæ•´æ€§")
            print("   4. é‡å»ºæ™‚åƒè€ƒ REBUILD_GUIDE.md")
            print("   5. é‡å»ºå¾Œé©—è­‰æ‰€æœ‰åŠŸèƒ½")
            
    except Exception as e:
        print(f"âŒ åŒ¯å‡ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    export_full_data()
